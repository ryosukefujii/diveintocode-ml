{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アンサンブル学習\n",
    "\n",
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "\n",
    "* ブレンディング\n",
    "* バギング\n",
    "* スタッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小さなデータセットの用意\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "\n",
    "この中の`train.csv`をダウンロードし、目的変数として`SalePrice`、説明変数として、`GrLivArea`と`YearBuilt`を使います。\n",
    "\n",
    "\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../house-prices-advanced-regression-techniques/train.csv')\n",
    "X = df[['GrLivArea', 'YearBuilt']]\n",
    "target = df.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "target = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "target = np.log(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.ravel(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 2), (1168, 2), (292, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## scikit-learn\n",
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "\n",
    "[sklearn.linear_model.LinearRegression — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "\n",
    "sklearn.svm.SVR — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n",
    "sklearn.tree.DecisionTreeRegressor — scikit-learn 0.21.3 documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ブレンディング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dtr_1',\n",
       " 0.06511442696822994,\n",
       " 'dtr_2',\n",
       " 0.06586510328914023,\n",
       " 'dtr_1, dtr_2',\n",
       " 0.05465309467360677)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同じ手法でパラメータを変えたインスタンスの組み合わせ\n",
    "dtr_1 = DecisionTreeRegressor(splitter='random')\n",
    "dtr_1.fit(X_train, y_train)\n",
    "dtr_pred_1 = dtr_1.predict(X_test)\n",
    "\n",
    "dtr_2 = DecisionTreeRegressor(splitter='best')\n",
    "dtr_2.fit(X_train, y_train)\n",
    "dtr_pred_2 = dtr_2.predict(X_test)\n",
    "\n",
    "# ブレンド\n",
    "blend = np.stack([dtr_pred_1, dtr_pred_2], 1)\n",
    "blend_pred = np.average(blend, axis=1)\n",
    "\n",
    "# 比較\n",
    "'dtr_1', mean_squared_error(y_test, dtr_pred_1), 'dtr_2', mean_squared_error(y_test, dtr_pred_2), 'dtr_1, dtr_2', mean_squared_error(y_test, blend_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('leg',\n",
       " 0.05846683591964387,\n",
       " 'dtr',\n",
       " 0.07841405750997235,\n",
       " 'leg, dtr',\n",
       " 0.056400550221579936)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ２つの異なる手法のブレンド\n",
    "\n",
    "leg = LinearRegression()\n",
    "leg.fit(X_train, y_train)\n",
    "leg_pred = leg.predict(X_test)\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train, y_train)\n",
    "dtr_pred = dtr.predict(X_test)\n",
    "\n",
    "# ブレンド\n",
    "blend = np.stack([leg_pred, dtr_pred], 1)\n",
    "blend_pred = np.average(blend, axis=1)\n",
    "\n",
    "# 比較\n",
    "'leg', mean_squared_error(y_test, leg_pred), 'dtr', mean_squared_error(y_test, dtr_pred), 'leg, dtr', mean_squared_error(y_test, blend_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('blend_1', 0.047575620801935045, 'blend_2', 0.041800499855544605)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ブレンドの比率を調整する\n",
    "leg = LinearRegression()\n",
    "leg.fit(X_train, y_train)\n",
    "leg_pred = leg.predict(X_test)\n",
    "\n",
    "svr = SVR(gamma='scale')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train, y_train)\n",
    "dtr_pred = dtr.predict(X_test)\n",
    "\n",
    "# ブレンド\n",
    "blend = np.stack([leg_pred, svr_pred, dtr_pred], 1)\n",
    "blend_pred = np.average(blend, axis=1)\n",
    "\n",
    "# ブレンド2\n",
    "rate = [0.1, 0.8, 0.1]\n",
    "blend_2 = np.stack([leg_pred*rate[0], svr_pred*rate[1], dtr_pred*rate[2]], 1)\n",
    "blend_pred_2 = np.sum(blend_2, axis=1)\n",
    "\n",
    "# 比較\n",
    "'blend_1', mean_squared_error(y_test, blend_pred), 'blend_2', mean_squared_error(y_test, blend_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ **ブートストラップサンプル** ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation]()\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(X.shape[0]*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.064415099440627\n",
      "1 0.08360751834300002\n",
      "2 0.06555633653447579\n",
      "avg 0.048255137396867015\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "n = 3\n",
    "row = int(X.shape[0]*0.2)\n",
    "pred_list = np.zeros((row, n))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, shuffle=True)\n",
    "\n",
    "for i in range(n):\n",
    "    index = np.random.choice(len(X_train), int(len(X_train)*0.8))\n",
    "    dtr = DecisionTreeRegressor(random_state=42)\n",
    "    dtr.fit(X_train[index], y_train[index])\n",
    "    pred_list[:,i] = dtr.predict(X_test)\n",
    "    \n",
    "avg = np.average(pred_list, axis=1)\n",
    "\n",
    "pred_list = np.hstack((pred_list, avg.reshape(-1, 1)))\n",
    "\n",
    "\n",
    "print(0, mean_squared_error(y_test, pred_list[:,0]))\n",
    "print(1, mean_squared_error(y_test, pred_list[:,1]))\n",
    "print(2, mean_squared_error(y_test, pred_list[:,2]))\n",
    "print('avg', mean_squared_error(y_test, pred_list[:,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スタッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは \n",
    "$K_{0}=3,M_{0}=2$程度にします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《学習時》\n",
    "\n",
    "\n",
    "（ステージ 0）\n",
    "\n",
    "\n",
    "* $学習データを K_{0}個に分割する。$\n",
    "* $分割した内の (K_{0}−1)個をまとめて学習用データ、残り 1 個を推定用データとする組み合わせが K_{0}個作れる。$\n",
    "* $あるモデルのインスタンスを K_{0} 個用意し、異なる学習用データを使い学習する。$\n",
    "* $それぞれの学習済みモデルに対して、使っていない残り 1 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）$\n",
    "* $さらに、異なるモデルのインスタンスも K_{0} 個用意し、同様のことを行う。モデルが M_{0}個あれば、 M_{0} 個のブレンドデータが得られる。$\n",
    "\n",
    "\n",
    "（ステージ n ）\n",
    "\n",
    "\n",
    "* $ステージ n−1のブレンドデータをM_{n}−1 次元の特徴量を持つ学習用データと考え、 K_{¥n}個に分割する。以下同様である。$\n",
    "\n",
    "（ステージ N ） ＊最後のステージ\n",
    "\n",
    "\n",
    "* $ステージ N−1の M_{N−1} 個のブレンドデータをM_{N−1}次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《推定時》\n",
    "\n",
    "\n",
    "（ステージ 0）\n",
    "\n",
    "\n",
    "* テストデータを K_{0}× M_{0} 個の学習済みモデルに入力し、K_{0} × M_{0} 個の推定値を得る。これを K_{0} の軸で平均値を求め M_{0}次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）$\n",
    "\n",
    "（ステージ n）\n",
    "\n",
    "\n",
    "* $ステージ n−1で得たブレンドテストを K_{n}×M_{n}個の学習済みモデルに入力し、K_{n}×M_{n}個の推定値を得る。\n",
    "これを K_{n}の軸で平均値を求め M_{0}次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）$\n",
    "\n",
    "（ステージ N ）＊最後のステージ\n",
    "\n",
    "* $ステージ N−1で得たブレンドテストを学習済みモデルに入力し、推定値を得る。$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('second_model', 0.041554366516274324)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stage 0\n",
    "y = target\n",
    "n = 10\n",
    "models = [DecisionTreeRegressor(), LinearRegression(), SVR(gamma='scale')]\n",
    "kf = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "\n",
    "cnt = 0\n",
    "cnt_model = 0\n",
    "blend_data = np.zeros((len(X_train), len(models)))\n",
    "# val_data = np.zeros((len(X_train), len(models)))\n",
    "blend_test = np.zeros((len(models), len(X_test), n))\n",
    "for i in range(len(models)):\n",
    "    predict_list = np.array([])\n",
    "    val_list =  np.array([])\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_bd_train, X_bd_test = X_train[train_index], X_train[test_index]\n",
    "        y_bd_train, y_bd_test = y_train[train_index], y_train[test_index]\n",
    "        models[i] \n",
    "        models[i].fit(X_bd_train, y_bd_train)\n",
    "        bd_pred = models[i].predict(X_bd_test)\n",
    "        bt_pred = models[i].predict(X_test)\n",
    "        predict_list = np.append(predict_list, bd_pred)\n",
    "#         val_list = np.append(val_list, y_bd_test)\n",
    "        blend_test[cnt_model, :, cnt] = bt_pred\n",
    "        cnt += 1                    \n",
    "    blend_data[:,i] = predict_list\n",
    "#     val_data = val_list\n",
    "    cnt_model += 1\n",
    "    cnt = 0\n",
    "\n",
    "\n",
    "\n",
    "# stage n\n",
    "blend_data_2 = np.array([])\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_bd_train_2, X_bd_test_2 = blend_data[train_index], blend_data[test_index]\n",
    "    y_bd_train_2, y_bd_test_2 = val_data[train_index], val_data[test_index]\n",
    "    model = models[0] \n",
    "    model.fit(X_bd_train_2, y_bd_train_2)\n",
    "    bd_pred_2 = model.predict(X_bd_test_2)\n",
    "    blend_data_2 = np.append(blend_data_2, bd_pred_2)\n",
    "\n",
    "blend_test = np.average(blend_test, axis=2).T # ブレンドテスト\n",
    "\n",
    "\n",
    "\n",
    "# last stage\n",
    "last_model = models[2] \n",
    "last_model.fit(X_bd_train_2, y_bd_train_2)\n",
    "last_pred = last_model.predict(blend_test)\n",
    "\n",
    "'second_model', mean_squared_error(y_test, last_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
