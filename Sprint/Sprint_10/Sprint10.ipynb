{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb # for debug\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train = X_train.reshape(-1, 784)\n",
    "# X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# X_train = X_train.astype(np.float)\n",
    "# X_test = X_test.astype(np.float)\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "# y_test = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "**畳み込みニューラルネットワーク（CNN）** のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "このSprintでは1次元の **畳み込み層** を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$\n",
    " : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$\n",
    " : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x(_{i+s})$\n",
    " : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$\n",
    " : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$\n",
    " : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$α$\n",
    "  : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$: $w_s$\n",
    "に関する損失 $L$の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$\n",
    " : \n",
    "$b$\n",
    " に関する損失 \n",
    "$L$\n",
    " の勾配\n",
    "\n",
    "\n",
    "勾配 \n",
    "$\\frac{\\partial L}{\\partial w_s}$や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$\n",
    " : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N\n",
    "_{o\n",
    "u\n",
    "t}$\n",
    " : 出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$\n",
    " : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、 \n",
    "$j−s<0$\n",
    " または \n",
    "$j−s>N_{out}−1$\n",
    " のとき \n",
    "$\\frac{\\partial L}{\\partial a_{(j-s)}} =0$\n",
    " です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.lr = 1\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.N_in = len(x) # 入力のサイズ（特徴量の数）\n",
    "        self.P = 0 # Padding\n",
    "        self.F = len(self.W) # フィルタのサイズ\n",
    "        self.S = 1 # stride\n",
    "        self.N_out = int((self.N_in + 2* self.P - self.F)/self.S + 1) # 𝑁𝑜𝑢𝑡  : 出力のサイズ（特徴量の数\n",
    "        \n",
    "        self.Z = x # 今回必要？\n",
    "        self.A = np.empty(self.N_out)\n",
    "        for i in range(self.F-1):\n",
    "            self.A[i] = sum(x[i:i + self.F]*self.W)+self.B\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = dA  # 今回必要？\n",
    "                \n",
    "        dZ = np.empty(self.N_in)\n",
    "        for j in range(self.N_in):\n",
    "            dZ_list = np.empty(self.F)\n",
    "            for i in range(self.F):\n",
    "                val = j - i\n",
    "                if val < 0 or val > self.N_out -1:\n",
    "                    dZ_list[i] = 0\n",
    "                else:\n",
    "                    dZ_list[i] = dA[val]*self.W[i]\n",
    "                dZ[j] = sum(dZ_list) \n",
    "        self.dZ = dZ\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.forward(X)\n",
    "        dA = np.array([10, 20])\n",
    "        self.backward(dA)\n",
    "        # w 更新\n",
    "        new_W = np.empty(self.F)\n",
    "        for i in range(self.F):\n",
    "            new_W[i] = sum(self.X[i:i+self.N_out]*dA)\n",
    "        self.W = new_W\n",
    "        # b 更新  \n",
    "        new_B = sum(dA)\n",
    "        self.B = new_B \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        self.forward_propagation_func(X)\n",
    "        return np.argmax(self.z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "\n",
    "$N\n",
    "_{o\n",
    "u\n",
    "t}$\n",
    " : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{\n",
    "i\n",
    "n}$\n",
    " : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$\n",
    " : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$\n",
    " : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$\n",
    " : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dA = np.array([10, 20]) # バックプロバゲーションここから始まる\n",
    "\n",
    "# # w 更新\n",
    "# w_new = np.empty(F)\n",
    "# for i in range(F):\n",
    "#     w_new[i] = sum(x[i:i+N_out]*dA)\n",
    "\n",
    "# w_new\n",
    "# # b 更新  \n",
    "# b_new = sum(dA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # delta_b = np.array([30])\n",
    "# # delta_w = np.array([50, 80, 110])\n",
    "# # delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_in = len(x) # 入力のサイズ（特徴量の数）\n",
    "# P = 0 # Padding\n",
    "# F = len(w) # フィルタのサイズ\n",
    "# S = 1 # stride\n",
    "# N_out = int((N_in + 2* P - F)/S + 1)\n",
    "# N_out # 𝑁𝑜𝑢𝑡  : 出力のサイズ（特徴量の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bk-proba\n",
    "# dZ = np.empty(N_in)\n",
    "# for j in range(N_in):\n",
    "#     dZ_list = np.empty(F)\n",
    "#     for i in range(F):\n",
    "#         val = j - i\n",
    "#         print(val)\n",
    "#         if val < 0 or val > N_out -1:\n",
    "#             dZ_list[i] = 0\n",
    "#         else:\n",
    "#             dZ_list[i] = dA[val]*w[i]\n",
    "#         dZ[j] = sum(dZ_list)\n",
    "# dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "\n",
    "入力$x$、重み$w$、バイアス$b$を次のようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションをすると出力は次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([35, 50])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20]) # バックプロバゲーションここから始まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックプロパゲーションをすると次のような値になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [35. 50.]\n",
      "delta_b: 30\n",
      "delta_w: [ 50.  80. 110.]\n",
      "delta_x: [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "sample = SimpleConv1d(w, b)\n",
    "y = np.array([30, 110, 170, 140]) # fit用の仮値\n",
    "sample.fit(x, y)\n",
    "print('a:', sample.A)\n",
    "print('delta_b:', sample.B)\n",
    "print('delta_w:', sample.W)\n",
    "print('delta_x:', sample.dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # fw-proba\n",
    "# A = np.empty(N_out)\n",
    "# for i in range(F-1):\n",
    "#     A[i] = sum(x[i:i + F]*w)+b\n",
    "# A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "\n",
    "$$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$$\n",
    "\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "\n",
    "**《参考》**\n",
    "\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "\n",
    "[Indexing — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x[:,0:2], axis=0)\n",
    "np.sum(x[:,0:3]*w[0,:,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力は次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "\n",
    "**《補足》**\n",
    "\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(`バッチサイズ`、`チャンネル数`、`特徴量数`)または(`バッチサイズ`、`特徴量数`、`チャンネル数`)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(`チャンネル数`、`特徴量数`)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_:　\n",
      " [[1 2 3 4]\n",
      " [2 3 4 5]]\n",
      "x_shape:　\n",
      " (2, 4)\n",
      "---------\n",
      "w_:　\n",
      " [[[1 1 2]\n",
      "  [2 1 1]]\n",
      "\n",
      " [[2 1 1]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]]]\n",
      "w_shape:　\n",
      " (3, 2, 3)\n",
      "---------\n",
      "b_:　\n",
      " [1 2 3]\n",
      "b_shape:　\n",
      " (3,)\n",
      "---------\n",
      "out_:　\n",
      " [[21 29]\n",
      " [18 25]\n",
      " [18 24]]\n",
      "out_shape:　\n",
      " (3, 2)\n",
      "---------\n",
      "loss_:　\n",
      " [[ 9 11]\n",
      " [32 35]\n",
      " [52 56]]\n",
      "loss_shape:　\n",
      " (3, 2)\n",
      "---------\n",
      "x_delta:　\n",
      " [[125 230 204 113]\n",
      " [102 206 195 102]]\n",
      "x_delta_shape:　\n",
      " (2, 4)\n",
      "---------\n",
      "w_delta:　\n",
      " [[[ 31  51  71]\n",
      "  [ 51  71  91]]\n",
      "\n",
      " [[102 169 236]\n",
      "  [169 236 303]]\n",
      "\n",
      " [[164 272 380]\n",
      "  [272 380 488]]]\n",
      "w_delta_shape:　\n",
      " (3, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "## フォワードプロパゲーション・バックプロパゲーションの確認用トイデータ\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_ = np.array([[1,2,3,4],\n",
    "               [2,3,4,5]])\n",
    "\n",
    "\n",
    "w_ = np.array([[[1,1,2],[2,1,1]],\n",
    "              [[2,1,1],[1,1,1]],\n",
    "              [[1,1,1],[1,1,1]]])\n",
    "\n",
    "\n",
    "b_ = np.array([1,2,3])\n",
    "\n",
    "# フォワードの出力\n",
    "out_ = np.array([[21,29],\n",
    "                [18,25],\n",
    "                [18,24]])\n",
    "\n",
    "\n",
    "loss_ = np.array([[9,11],\n",
    "                [32,35],\n",
    "                [52,56]])\n",
    "\n",
    "\n",
    "# バックワードの勾配\n",
    "x_delta = np.array([[125,230,204,113],\n",
    "                    [102,206,195,102]])\n",
    "\n",
    "\n",
    "w_delta = np.array([[[31,51,71],[51,71,91]],\n",
    "                    [[102,169,236],[169,236,303]],\n",
    "                    [[164,272,380],[272,380,488]]])\n",
    "\n",
    "\n",
    "print('x_:　\\n',x_)\n",
    "print('x_shape:　\\n',x_.shape)\n",
    "print('---------')\n",
    "print('w_:　\\n',w_)\n",
    "print('w_shape:　\\n',w_.shape)\n",
    "print('---------')\n",
    "print('b_:　\\n',b_)\n",
    "print('b_shape:　\\n',b_.shape)\n",
    "print('---------')\n",
    "print('out_:　\\n',out_)\n",
    "print('out_shape:　\\n',out_.shape)\n",
    "print('---------')\n",
    "print('loss_:　\\n',loss_)\n",
    "print('loss_shape:　\\n',loss_.shape)\n",
    "print('---------')\n",
    "print('x_delta:　\\n',x_delta)\n",
    "print('x_delta_shape:　\\n',x_delta.shape)\n",
    "print('---------')\n",
    "print('w_delta:　\\n',w_delta)\n",
    "print('w_delta_shape:　\\n',w_delta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 31.,  51.,  71.],\n",
       "        [ 51.,  71.,  91.]],\n",
       "\n",
       "       [[102., 169., 236.],\n",
       "        [169., 236., 303.]],\n",
       "\n",
       "       [[164., 272., 380.],\n",
       "        [272., 380., 488.]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_new = np.empty((3, 2, 3))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        w_new[:,i,j] = np.sum(x_[i, j:j+2]*loss_, axis=1)\n",
    "        \n",
    "w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.lr = 1\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.N_in = x.shape[1] # 入力のサイズ（特徴量の数）...x.shape[1]に変更\n",
    "        self.P = 0 # Padding\n",
    "        self.F = self.W.shape[2] # フィルタのサイズ ...self.W.shape[2]に変更\n",
    "        self.S = 1 # stride\n",
    "        self.N_out = int((self.N_in + 2* self.P - self.F)/self.S + 1) # 𝑁𝑜𝑢𝑡  : 出力のサイズ（特徴量の数）\n",
    "        self.Z = x # 今回必要？\n",
    "        \n",
    "        self.A = np.empty((len(self.B), self.N_out))\n",
    "        for j in range(len(self.B)):\n",
    "            for i in range(self.N_out):\n",
    "                self.A[j,i] = np.sum(x[:,i:i + self.F]*self.W[j]) + self.B[j]  \n",
    "        print(self.A)\n",
    "        return self.A\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = dA  # 今回必要？\n",
    "        \n",
    "        dZ = np.empty((self.X.shape[0], self.N_in)) # (2, 4)\n",
    "        for i in range(self.X.shape[0]): # 2→入力ch \n",
    "            for j in range(self.N_in): # N_in = ４→Xの特徴量\n",
    "                output_ch_list = np.empty((self.W.shape[0], self.F)) #(3, 3)\n",
    "                for k in range(self.W.shape[0]): # 3→w.shape[0]→出力ch\n",
    "                    for l in range(self.F): # Wの数 = 3\n",
    "                        val = j - l\n",
    "                        if val < 0 or val > self.N_out -1:\n",
    "                            output_ch_list[k, l] = 0\n",
    "                        else:\n",
    "                            output_ch_list[k, l] = dA[k,val]*self.W[k,i,l] \n",
    "                dZ[i, j] = np.sum(output_ch_list)\n",
    "        print(dZ)\n",
    "#         pdb.set_trace() \n",
    "        self.dZ = dZ       \n",
    "\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.forward(X)\n",
    "        dA = np.array( [[ 9, 11],[32, 35],[52, 56]])\n",
    "        self.backward(dA)\n",
    "#         pdb.set_trace() \n",
    "        # w 更新\n",
    "        new_W = np.empty((self.W.shape[0], self.W.shape[1], self.F))\n",
    "        for i in range(self.W.shape[1]):\n",
    "            for j in range(self.F):\n",
    "#                 pdb.set_trace()\n",
    "                new_W[:, i, j] = np.sum(self.X[i, j:j+self.N_out]*dA, axis=1)\n",
    "        self.W = new_W\n",
    "        \n",
    "        \n",
    "        # b 更新  \n",
    "        new_B = np.sum(dA, axis=1)\n",
    "        self.B = new_B \n",
    "#         pdb.set_trace() \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        self.forward_propagation_func(X)\n",
    "        return np.argmax(self.z3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21. 29.]\n",
      " [18. 25.]\n",
      " [18. 24.]]\n",
      "[[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n",
      "a: [[21. 29.]\n",
      " [18. 25.]\n",
      " [18. 24.]]\n",
      "delta_b: [ 20  67 108]\n",
      "delta_w: [[[ 31.  51.  71.]\n",
      "  [ 51.  71.  91.]]\n",
      "\n",
      " [[102. 169. 236.]\n",
      "  [169. 236. 303.]]\n",
      "\n",
      " [[164. 272. 380.]\n",
      "  [272. 380. 488.]]]\n",
      "delta_x: [[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n"
     ]
    }
   ],
   "source": [
    "sample = Conv1d(w_, b_)\n",
    "y = np.array([30, 110, 170, 140]) # fit用の仮値\n",
    "sample.fit(x_, y)\n",
    "print('a:', sample.A)\n",
    "print('delta_b:', sample.B)\n",
    "print('delta_w:', sample.W)\n",
    "print('delta_x:', sample.dZ)\n",
    "\n",
    "# aの答え\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "\n",
    "numpy.pad — NumPy v1.17 Manual\n",
    "\n",
    "\n",
    "【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。\n",
    "\n",
    "\n",
    "【問題7】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 **平滑化** を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train = X_train.reshape(-1, 784)\n",
    "# X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=42):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # layerの引数は\"class FCのインスタンス自体\"を取得している。\n",
    "\n",
    "        layer.B -= self.lr*layer.B_dash # B更新\n",
    "        layer.W -= self.lr*layer.W_dash # W更新\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # layerの引数は\"class FCのインスタンス自体\"を取得している。\n",
    "        H_B =layer.H_B + layer.B_dash**2\n",
    "        H_W =layer.H_W + layer.W_dash**2 \n",
    "        layer.B -= self.lr/np.sqrt(H_B + 0.1)*layer.B_dash # B更新\n",
    "        layer.W -= self.lr/np.sqrt(H_W + 0.1)*layer.W_dash # W更新\n",
    "        layer.H_B = H_B\n",
    "        layer.H_W = H_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def cross_entropy_func(self, x, y):\n",
    "        li = np.empty(len(x))\n",
    "        for i in range(len(x)):\n",
    "            lj = np.empty(10) # 10 = self.n_output, 最終的に変数にする\n",
    "            for j in range(10): # 10 = self.n_output, 最終的に変数にする\n",
    "                lj[j] = y[i,j]*np.log(x[i,j])\n",
    "            li[i] = sum(lj)\n",
    "        self.l = sum(li)/-10 # 10 = self.n_output, 最終的に変数にする\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_max = np.max(x, axis=1)\n",
    "        exp_x = np.exp(x - x_max.reshape(-1, 1))\n",
    "        sum_exp_x = np.sum(exp_x, axis=1).reshape(-1, 1)  \n",
    "        return exp_x/sum_exp_x\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        self.cross_entropy_func(x, y)\n",
    "        return x - y\n",
    "\n",
    "\n",
    "      \n",
    "class Tanh:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.A = x # backwardの引数のためのAを保管\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return x*(1 - np.tanh(self.A)**2)\n",
    "\n",
    "class Sigmoid:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.A = x # backwardの引数のためのAを保管\n",
    "        return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "    def backward(self, x):\n",
    "        return x*(1 - np.tanh(self.A)**2)\n",
    "    \n",
    "class ReLU:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.F = np.where(x > 0, 1, 0) # backwardの引数のためのAを保管\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return x*self.F\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Xavierの初期値\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = 1/np.sqrt(n_nodes1) # self.batch_sizeかも\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "    \n",
    "class He:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Xavierの初期値\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = np.sqrt(2/n_nodes1) \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.lr = 1\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.N_in = x.shape[1] # 入力のサイズ（特徴量の数）...x.shape[1]に変更\n",
    "        self.P = 0 # Padding\n",
    "        self.F = self.W.shape[2] # フィルタのサイズ ...self.W.shape[2]に変更\n",
    "        self.S = 1 # stride\n",
    "        self.N_out = int((self.N_in + 2* self.P - self.F)/self.S + 1) # 𝑁𝑜𝑢𝑡  : 出力のサイズ（特徴量の数）\n",
    "        self.Z = x # 今回必要？\n",
    "        \n",
    "        self.A = np.empty((len(self.B), self.N_out))\n",
    "        for j in range(len(self.B)):\n",
    "            for i in range(self.N_out):\n",
    "                self.A[j,i] = np.sum(x[:,i:i + self.F]*self.W[j]) + self.B[j]  \n",
    "        print(self.A)\n",
    "        return self.A\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.dA = dA  # 今回必要？\n",
    "        \n",
    "        dZ = np.empty((self.X.shape[0], self.N_in)) # (2, 4)\n",
    "        for i in range(self.X.shape[0]): # 2→入力ch \n",
    "            for j in range(self.N_in): # N_in = ４→Xの特徴量\n",
    "                output_ch_list = np.empty((self.W.shape[0], self.F)) #(3, 3)\n",
    "                for k in range(self.W.shape[0]): # 3→w.shape[0]→出力ch\n",
    "                    for l in range(self.F): # Wの数 = 3\n",
    "                        val = j - l\n",
    "                        if val < 0 or val > self.N_out -1:\n",
    "                            output_ch_list[k, l] = 0\n",
    "                        else:\n",
    "                            output_ch_list[k, l] = dA[k,val]*self.W[k,i,l] \n",
    "                dZ[i, j] = np.sum(output_ch_list)\n",
    "        print(dZ)\n",
    "#         pdb.set_trace() \n",
    "        self.dZ = dZ       \n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_:\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=None, batch_size=20, a=0.01, n_epoch=5):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = a\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        self.n_features = 784\n",
    "        self.n_nodes1 = 400\n",
    "        self.n_nodes2 = 200\n",
    "        self.n_output = 10\n",
    "        self.sigma = 0.01 # ガウス分布の標準偏差\n",
    "\n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "        \n",
    "# fitメソッド内        \n",
    "        optimizer = SGD(self.lr)\n",
    "        self.Conv1d_1 = Conv1d(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.activation1 = Tanh()\n",
    "#         self.Conv1d_2 = Conv1d(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.activation2 = Tanh()\n",
    "#         self.Conv1d_3 = Conv1d(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size = self.batch_size)\n",
    "        self.l_list = np.empty(self.n_epoch)\n",
    "        self.l_val_list = np.empty(self.n_epoch)\n",
    "        self.cnt = 0\n",
    "        self.cnt_list = []\n",
    "        for i in range(self.n_epoch):\n",
    "            for X, y in get_mini_batch:        \n",
    "        \n",
    "        \n",
    "# イテレーションごとのフォワード    \n",
    "                A1 = self.Conv1d_1.forward(X)\n",
    "#                 A1 = self.FC1.forward(X)\n",
    "#                 Z1 = self.activation1.forward(A1)\n",
    "#                 A2 = self.FC2.forward(Z1)\n",
    "#                 Z2 = self.activation2.forward(A2)\n",
    "#                 A3 = self.FC3.forward(Z2)\n",
    "#                 Z3 = self.activation3.forward(A3)\n",
    "                Z3 = self.activation3.forward(A1)\n",
    "\n",
    "# イテレーションごとのバックワード\n",
    "                dA3 = self.activation3.backward(Z3, y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "#                 dZ2 = self.FC3.backward(dA3)\n",
    "#                 dA2 = self.activation2.backward(dZ2)\n",
    "#                 dZ1 = self.FC2.backward(dA2)\n",
    "#                 dA1 = self.activation1.backward(dZ1)\n",
    "#                 dZ0 = self.FC1.backward(dA1) # dZ0は使用しない   \n",
    "                dZ0 = self.FC1.backward(dA3) # dZ0は使用しない        \n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d_1.forward(X)\n",
    "        Z3 = self.activation3.forward(A1)\n",
    "#         A1 = self.FC1.forward(X)\n",
    "#         Z1 = self.activation1.forward(A1)\n",
    "#         A2 = self.FC2.forward(Z1)\n",
    "#         Z2 = self.activation2.forward(A2)\n",
    "#         A3 = self.FC3.forward(Z2)\n",
    "#         Z3 = self.activation3.forward(A3)\n",
    "        \n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-259d09632cf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1d_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-ffb3647e886e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# fitメソッド内\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#         self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         self.activation1 = Tanh()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "model = Conv1d_()\n",
    "model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-ed298286ea60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAF6CAYAAAAeZ/GvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXaElEQVR4nO3df2yV9dn48eswJqTTBIanNDELWWIGj9iK2bIhGjKdtoq1jh+bohN/rcqcaYaJ0QGjyzbEqRtuzpg1WWbMIIFkG9D9UTo1qLNsBqPDMAkyYkAjbaFOQctSPPf3j2V9Hr6ordCb0376eiX+cXOfnnN5pfrm3Nz0FLIsywIASNaYcg8AAORL7AEgcWIPAIkTewBInNgDQOLEHgASl3vsDx8+HPX19fHGG28cd+7VV1+NefPmRV1dXSxbtiyOHj2a9zgAMOrkGvu///3vsXDhwnj99dc/9Pzdd98dK1asiM2bN0eWZbF+/fo8xwGAUSnX2K9fvz6am5ujsrLyuHNvvvlmHDlyJGbMmBEREfPmzYu2trY8xwGAUWlsnk++cuXKjzzX1dUVxWKx/7hYLEZnZ2ee4wDAqFS2G/RKpVIUCoX+4yzLjjkGAIZGru/sP05VVVV0d3f3Hx84cOBDL/d/nLfffi9KJT/aPy+TJp0eBw8eLvcYybPn/Nlx/uw4f2PGFGLixM+c0NeWLfZnnXVWjBs3Ll588cX44he/GBs3bozZs2d/oucolTKxz5n9nhr2nD87zp8dD1+n/DJ+Y2NjvPLKKxER8dBDD8WqVavi8ssvj/fffz8WLVp0qscBgOQVRvJH3B48eNjvJHNULJ4R3d2Hyj1G8uw5f3acPzvO35gxhZg06fQT+9ohngUAGGbEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEhcrrFvbW2NOXPmRG1tbaxZs+a48zt27Ij58+dHQ0ND3H777fHuu+/mOQ4AjEq5xb6zszNWr14da9eujQ0bNsS6deti9+7dxzxm5cqV0dTUFJs2bYrPf/7z8Zvf/CavcQBg1Mot9h0dHTFz5syYMGFCVFRURF1dXbS1tR3zmFKpFO+9915ERPT29sb48ePzGgcARq3cYt/V1RXFYrH/uLKyMjo7O495zL333hvLly+Piy66KDo6OuLaa6/NaxwAGLXG5vXEpVIpCoVC/3GWZcccHzlyJJYtWxaPP/541NTUxG9/+9u45557oqWlZdCvMWnS6UM6M8crFs8o9wijgj3nz47zZ8fDV26xr6qqim3btvUfd3d3R2VlZf/xrl27Yty4cVFTUxMREddcc0384he/+ESvcfDg4SiVsqEZmOMUi2dEd/ehco+RPHvOnx3nz47zN2ZM4YTf5OZ2GX/WrFmxdevW6Onpid7e3mhvb4/Zs2f3n58yZUrs378/9uzZExERTz31VFRXV+c1DgCMWrm9s588eXIsWbIkFi1aFH19fbFgwYKoqamJxsbGaGpqiurq6li1alV873vfiyzLYtKkSXHfffflNQ4AjFqFLMtG7HVwl/Hz5bLcqWHP+bPj/Nlx/oblZXwAYHgQewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8Aics19q2trTFnzpyora2NNWvWHHd+z549ccMNN0RDQ0Pceuut8c477+Q5DgCMSrnFvrOzM1avXh1r166NDRs2xLp162L37t3957Msi+985zvR2NgYmzZtiv/5n/+JlpaWvMYBgFErt9h3dHTEzJkzY8KECVFRURF1dXXR1tbWf37Hjh1RUVERs2fPjoiIxYsXx/XXX5/XOAAwauUW+66urigWi/3HlZWV0dnZ2X+8d+/eOPPMM2Pp0qUxd+7caG5ujoqKirzGAYBRa2xeT1wqlaJQKPQfZ1l2zPHRo0fjhRdeiN/97ndRXV0dDz/8cNx///1x//33D/o1Jk06fUhn5njF4hnlHmFUsOf82XH+7Hj4yi32VVVVsW3btv7j7u7uqKys7D8uFosxZcqUqK6ujoiI+vr6aGpq+kSvcfDg4SiVsqEZmOMUi2dEd/ehco+RPHvOnx3nz47zN2ZM4YTf5OZ2GX/WrFmxdevW6Onpid7e3mhvb+//8/mIiPPPPz96enpi586dERHx9NNPx/Tp0/MaBwBGrdze2U+ePDmWLFkSixYtir6+vliwYEHU1NREY2NjNDU1RXV1dTz66KOxfPny6O3tjaqqqnjggQfyGgcARq1ClmUj9jq4y/j5clnu1LDn/Nlx/uw4f8PyMj4AMDyIPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxg4r9gQMH4qmnnoqIiAcffDBuvPHG2LlzZ66DAQBDY1Cxv/fee2Pfvn2xdevWeO655+Lqq6+On/zkJ3nPBgAMgUHF/l//+lfcdNNN8eyzz0Z9fX3Mmzcvent7854NABgCg4p9X19f9PX1xXPPPRezZs2K3t7eeP/99/OeDQAYAoOK/de+9rW44IILYuLEiXHuuefGN77xjaivr897NgBgCBSyLMsG88D9+/fH5MmTo1AoxM6dO2PatGl5zzaggwcPR6k0qPE5AcXiGdHdfajcYyTPnvNnx/mz4/yNGVOISZNOP7GvHcyDDhw4EDt27IhCoRAPPvhgrFq1yt34ADBCuBsfABLnbnwASJy78QEgce7GB4DEfaK78auqqiIi3I0/Sri79tSw5/zZcf7sOH8nczf+2ME8qFQqRWtrazz77LNx9OjRuPDCC+Pss8+OsWMH9eUAQBkN6jL+z372s/jrX/8aN954Y9x8883x0ksvxQMPPJD3bADAEBjUW/Pnnnsufv/738enP/3piIj46le/Gg0NDbF06dJchwMATt6g3tlnWdYf+oiI00477ZhjAGD4GlTsp02bFvfdd1/s3bs39u3bF/fdd1984QtfyHs2AGAIDCr2zc3N8e6778bChQvjm9/8Zrz99tuxYsWKvGcDAIbAx/6Z/VVXXXXM8Wc/+9mI+M9fvfvWt74Vra2t+U0GAAyJj439D37wg1M1BwCQk4+N/Ze//OVTNQcAkJNB/Zk9ADByiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJC4XGPf2toac+bMidra2lizZs1HPm7Lli1xySWX5DkKAIxaY/N64s7Ozli9enX84Q9/iNNOOy2uvfba+MpXvhJnn332MY87cOBA/PSnP81rDAAY9XJ7Z9/R0REzZ86MCRMmREVFRdTV1UVbW9txj1u+fHnceeedeY0BAKNebrHv6uqKYrHYf1xZWRmdnZ3HPOaJJ56Ic845J84777y8xgCAUS+3y/ilUikKhUL/cZZlxxzv2rUr2tvb4/HHH4/9+/ef0GtMmnT6Sc/JxysWzyj3CKOCPefPjvNnx8NXbrGvqqqKbdu29R93d3dHZWVl/3FbW1t0d3fH/Pnzo6+vL7q6uuK6666LtWvXDvo1Dh48HKVSNqRz87+KxTOiu/tQucdInj3nz47zZ8f5GzOmcMJvcnO7jD9r1qzYunVr9PT0RG9vb7S3t8fs2bP7zzc1NcXmzZtj48aN0dLSEpWVlZ8o9ADA4OQW+8mTJ8eSJUti0aJF8fWvfz3q6+ujpqYmGhsb45VXXsnrZQGA/08hy7IRex3cZfx8uSx3athz/uw4f3acv2F5GR8AGB7EHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEhcrrFvbW2NOXPmRG1tbaxZs+a4808++WRcffXV0dDQEHfccUe88847eY4DAKNSbrHv7OyM1atXx9q1a2PDhg2xbt262L17d//5w4cPxw9/+MNoaWmJTZs2xdSpU+ORRx7JaxwAGLVyi31HR0fMnDkzJkyYEBUVFVFXVxdtbW395/v6+qK5uTkmT54cERFTp06Nt956K69xAGDUyi32XV1dUSwW+48rKyujs7Oz/3jixIlx2WWXRUTEkSNHoqWlJS699NK8xgGAUWtsXk9cKpWiUCj0H2dZdszxfx06dCi++93vxrRp02Lu3Lmf6DUmTTr9pOfk4xWLZ5R7hFHBnvNnx/mz4+Ert9hXVVXFtm3b+o+7u7ujsrLymMd0dXXFrbfeGjNnzoylS5d+4tc4ePBwlErZSc/KhysWz4ju7kPlHiN59pw/O86fHedvzJjCCb/Jze0y/qxZs2Lr1q3R09MTvb290d7eHrNnz+4//8EHH8TixYvjiiuuiGXLln3ou34A4OTl9s5+8uTJsWTJkli0aFH09fXFggULoqamJhobG6OpqSn2798f//jHP+KDDz6IzZs3R0TEueeeGytXrsxrJAAYlQpZlo3Y6+Au4+fLZblTw57zZ8f5s+P8DcvL+ADA8CD2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DEiT0AJE7sASBxYg8AiRN7AEic2ANA4sQeABIn9gCQOLEHgMSJPQAkTuwBIHFiDwCJE3sASJzYA0DixB4AEif2AJA4sQeAxIk9ACRO7AEgcWIPAIkTewBInNgDQOJyjX1ra2vMmTMnamtrY82aNcedf/XVV2PevHlRV1cXy5Yti6NHj+Y5DgCMSrnFvrOzM1avXh1r166NDRs2xLp162L37t3HPObuu++OFStWxObNmyPLsli/fn1e4wDAqJVb7Ds6OmLmzJkxYcKEqKioiLq6umhra+s//+abb8aRI0dixowZERExb968Y84DAENjbF5P3NXVFcVisf+4srIytm/f/pHni8VidHZ2fqLXGDOmcPKD8rHs+NSw5/zZcf7sOF8ns9/cYl8qlaJQ+N/Bsiw75nig84MxceJnTn5QPtakSaeXe4RRwZ7zZ8f5s+PhK7fL+FVVVdHd3d1/3N3dHZWVlR95/sCBA8ecBwCGRm6xnzVrVmzdujV6enqit7c32tvbY/bs2f3nzzrrrBg3bly8+OKLERGxcePGY84DAEOjkGVZlteTt7a2xq9//evo6+uLBQsWRGNjYzQ2NkZTU1NUV1fHzp07Y/ny5XH48OGYPn16rFq1Kk477bS8xgGAUSnX2AMA5ecn6AFA4sQeABIn9gCQOLEHgMSJPQAkbtjH3ifn5W+gHT/55JNx9dVXR0NDQ9xxxx3xzjvvlGHKkW+gPf/Xli1b4pJLLjmFk6VjoB3v2bMnbrjhhmhoaIhbb73V9/IJGGjHO3bsiPnz50dDQ0Pcfvvt8e6775ZhypHv8OHDUV9fH2+88cZx506oe9kwtn///uziiy/O3n777ey9997Lrrrqquy111475jFXXnll9tJLL2VZlmXf//73szVr1pRj1BFroB0fOnQou/DCC7P9+/dnWZZlDz/8cPbjH/+4XOOOWIP5Xs6yLOvu7s4uv/zy7OKLLy7DlCPbQDsulUpZbW1t9swzz2RZlmUPPvhg9sADD5Rr3BFpMN/HCxcuzLZs2ZJlWZatWrUq+/nPf16OUUe0l19+Oauvr8+mT5+e7du377jzJ9K9Yf3O3ifn5W+gHff19UVzc3NMnjw5IiKmTp0ab731VrnGHbEG2vN/LV++PO68884yTDjyDbTjHTt2REVFRf9P6ly8eHFcf/315Rp3RBrM93GpVIr33nsvIiJ6e3tj/Pjx5Rh1RFu/fn00Nzd/6I+QP9HuDevYf9gn5/3fT8Ybik/OG+0G2vHEiRPjsssui4iII0eOREtLS1x66aWnfM6RbqA9R0Q88cQTcc4558R55513qsdLwkA73rt3b5x55pmxdOnSmDt3bjQ3N0dFRUU5Rh2xBvN9fO+998by5cvjoosuio6Ojrj22mtP9Zgj3sqVK+NLX/rSh5470e4N69ifik/OG+0Gu8NDhw7FbbfdFtOmTYu5c+eeyhGTMNCed+3aFe3t7XHHHXeUY7wkDLTjo0ePxgsvvBALFy6MP/7xj/G5z30u7r///nKMOmINtOMjR47EsmXL4vHHH4+//OUvcd1118U999xTjlGTdaLdG9ax98l5+RtoxxH/+Z3kddddF1OnTo2VK1ee6hGTMNCe29raoru7O+bPnx+33XZb/84ZvIF2XCwWY8qUKVFdXR0REfX19bF9+/ZTPudINtCOd+3aFePGjYuampqIiLjmmmvihRdeOOVzpuxEuzesY++T8/I30I4/+OCDWLx4cVxxxRWxbNkyV05O0EB7bmpqis2bN8fGjRujpaUlKisrY+3atWWceOQZaMfnn39+9PT0xM6dOyMi4umnn47p06eXa9wRaaAdT5kyJfbv3x979uyJiIinnnqq/zdXDI0T7t7Q3T+Yj02bNmVXXnllVltbm7W0tGRZlmXf/va3s+3bt2dZlmWvvvpqNn/+/Kyuri676667sn//+9/lHHdE+rgdt7e3Z1OnTs0aGhr6/1m6dGmZJx6ZBvpe/q99+/a5G/8EDbTjl19+OZs/f342Z86c7JZbbskOHDhQznFHpIF2vGXLluyqq67K6uvrsxtvvDHbu3dvOccd0S6++OL+u/FPtns+9Q4AEjesL+MDACdP7AEgcWIPAIkTewBInNgDQOLEHjjOK6+8Ek1NTbF9+/ZYsWJFuccBTpLYA8eprq6OX/7yl7F7926fNwEJ8PfsgeP87W9/6/+c7EOHDkVtbW2sWrUqnn766Xjssceir68vxo8fH/fcc0+cf/758cgjj8TLL78cXV1dMXXq1HjooYfK/a8A/B9jyz0AMDyNHz8+brnllti8eXOsWrUqXn/99Vi9enU88cQTMXHixHjttdfi5ptvjvb29oj4z0dv/ulPf4qxY/1vBYYb/1UCg/L8889HV1dX3HTTTf2/VigUYu/evRERMWPGDKGHYcp/mcCglEqluOCCC+Lhhx/u/7W33norKisr489//rPPhodhzA16wEf61Kc+FUePHo2IiAsuuCCef/75+Oc//xkREc8880w0NDTEkSNHyjkiMAje2QMfacaMGfHoo4/GnXfeGb/61a/iRz/6Udx1112RZVmMHTs2HnvssfjMZz5T7jGBAbgbHwAS5zI+ACRO7AEgcWIPAIkTewBInNgDQOLEHgASJ/YAkDixB4DE/T//Ow4gR+OHIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.set()\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(model.cnt_list, model.l_list, label='train')\n",
    "plt.plot(model.cnt_list, model.l_val_list, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9670833333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800833333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_tr = model.predict(X_train)\n",
    "y_true_tr = np.argmax(y_train, axis=1)\n",
    "\n",
    "print(accuracy_score(y_true_tr, y_pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
