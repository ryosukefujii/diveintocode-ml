{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/FUZZY/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb # for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIYASU code\n",
    "x_ = np.array([[[[1,2,3,4]],\n",
    "                [[2,3,4,5]]]]*10) # (10, 2, 1, 4)\n",
    "w = np.array([[[[1,1,2]],[[2,1,1]]],\n",
    "                   [[[2,1,1]],[[1,1,1]]],\n",
    "                   [[[1,1,1]],[[1,1,1]]]])\n",
    "b = [[[1]],[[2]],[[3]]]\n",
    "loss = np.array([[[9,11]],\n",
    "                [[32,35]],\n",
    "                [[52,56]]])\n",
    "# 確認用\n",
    "out_ = np.array([[21,29],\n",
    "                [18,25],\n",
    "                [18,24]])\n",
    "x_delta = np.array([[125,230,204,113],\n",
    "                    [102,206,195,102]])\n",
    "w_delta = np.array([[[31,51,71],[51,71,91]],\n",
    "                    [[102,169,236],[169,236,303]],\n",
    "                    [[164,272,380],[272,380,488]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "\n",
    "`(n_samples, n_channels, height, width)` の`NCHW`または`(n_samples, height, width, n_channels)`の`NHWC`どちらかの形にしてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,np.newaxis, :,:, ]\n",
    "X_test = X_test[:,np.newaxis, :,:, ]\n",
    "\n",
    "# 画像サイズ縮小処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.transform(y_test[:, np.newaxis])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】2次元畳み込み層の作成\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$\n",
    "\n",
    "$a\n",
    "_{i,j,m}$\n",
    " : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "\n",
    "$i$\n",
    " : 配列の行方向のインデックス\n",
    "\n",
    "\n",
    "$j$\n",
    " : 配列の列方向のインデックス\n",
    "\n",
    "\n",
    "$m$\n",
    " : 出力チャンネルのインデックス\n",
    "\n",
    "\n",
    "$K$\n",
    " : 入力チャンネル数\n",
    "\n",
    "\n",
    "$F_h\n",
    ",F_w$\n",
    " : 高さ方向（$h$）と幅方向（$w$）のフィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s),(j+t),k}$\n",
    " : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "\n",
    "$w_{s,t,k,m}$\n",
    " : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "\n",
    "b_m\n",
    " : mチャンネルへの出力のバイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "\n",
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$\n",
    "\n",
    "$α$\n",
    "  : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{s,t,k,m}} : \n",
    "w_{s,t,k,m}$ に関する損失 $L$の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_{m}}: b_m$\n",
    " に関する損失 \n",
    "$L$\n",
    " の勾配\n",
    "\n",
    "\n",
    "勾配 \n",
    "$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$\n",
    " や $\\frac{\\partial L}{\\partial b_{m}}$\n",
    " を求めるためのバックプロパゲーションの数式が以下である。\n",
    " \n",
    " $$\n",
    " \\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    " $$\n",
    " \n",
    "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi行j列、mチャンネルの値\n",
    "\n",
    "$N_{out,h},N_{out,w}$: 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$: 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "\n",
    "$M$\n",
    "  : 出力チャンネル数\n",
    "  \n",
    "ただし、 \n",
    "$i−s<0$\n",
    " または $i−s>N_{out,h}−1$\n",
    " または $j−t<0$\n",
    " または $j−t>N_{out,w}−1$\n",
    " のとき  \n",
    " $\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}} =0$です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, x_shape, FN):\n",
    "        self.FN = FN\n",
    "        N, CH, H, W = x_shape # X.shape\n",
    "        # Padding \n",
    "        self.P = 0\n",
    "        # stride\n",
    "        self.S = 1 \n",
    "        # Learning Rate\n",
    "        self.lr = 1 \n",
    "        # Weight\n",
    "        FH, FW = 3, 3 # filter size\n",
    "        self.W = np.ones((FN, CH, FH, FW))\n",
    "        #print('W.shape', self.W.shape)\n",
    "\n",
    "        # Bias\n",
    "        self.B = np.ones(CH)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.x = x # for backward\n",
    "        N, CH, H, W = x.shape\n",
    "        FN, CH, FH, FW = self.W.shape\n",
    "#         pdb.set_trace()\n",
    "        H_out, W_out = int((H + 2*self.P - FH)/self.S + 1), int((W + 2*self.P - FW)/self.S + 1)\n",
    "        # Feature Map(N, CH, H_out, W_out)\n",
    "        self.FM = np.empty((N, FN, H_out, W_out)) \n",
    "        #print('FM.shape', self.FM.shape)\n",
    "        for i in range(N): # loop for n_sample times\n",
    "            for j in range(FN): # loop for n_filter times\n",
    "                for k in range(H_out):\n",
    "                    for l in range(W_out):\n",
    "#                         pdb.set_trace()\n",
    "                        #print()\n",
    "                        self.FM[i, j, k, l] = np.sum(x[i, :, k:k+FH, l:l+FW]  * self.W[j, :, :, :]) + self.B\n",
    "                        #print(self.FM.shape)\n",
    "        return self.FM\n",
    "\n",
    "    \n",
    "    def backward(self, dA): # dA(20, 32, 26, 26) -> X(20, 1, 28, 28)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "#         pdb.set_trace()\n",
    "        N, CH, H, W = self.x.shape\n",
    "        FN, CH, FH, FW = self.W.shape\n",
    "        dZ = np.empty((self.x.shape))\n",
    "        for i in range(N): # 2→入力ch \n",
    "            for j in range(CH): # N_in = ４→Xの特徴量\n",
    "                for k in range(FH): # 3→w.shape[0]→出力ch\n",
    "                    for l in range(FW): # Wの数 = 3\n",
    "                        dZ[i, j, k:k + FH, l:l + FW] += np.sum(dA[i, j, k, l]* self.W[j, :, :, :]) + self.B \n",
    "\n",
    "#         pdb.set_trace() \n",
    "        self.dZ = dZ           \n",
    "        self.N_out = int((CH + 2* self.P - FN)/self.S + 1)\n",
    "        \n",
    "        # w 更新\n",
    "        new_W = np.ones(self.W.shape) # new_W.shape (32, 1, 3, 3)\n",
    "        for i in range(N): # 2→入力ch \n",
    "            for j in range(CH): # N_in = ４→Xの特徴量\n",
    "                for k in range(FH): # 3→w.shape[0]→出力ch\n",
    "                    for l in range(FW): # Wの数 = 3\n",
    "                        new_W[i, j, k, l] = np.sum(self.x[i, :, k:k+FH, l:l+FW]*dA[i, j, k, l])\n",
    "        self.W -= self.lr*new_W\n",
    "        \n",
    "        # b 更新  \n",
    "        new_B = np.ones(self.B.shape)\n",
    "        for i in range(self.x.shape[1]):\n",
    "            new_B[i] = np.sum(dA[:,i,:,:])\n",
    "        self.B -= self.lr*new_B     \n",
    "    \n",
    "    \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$\n",
    "\n",
    "$N_{out}$\n",
    "  : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$\n",
    " : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$\n",
    " : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$\n",
    " : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$\n",
    " : ストライドのサイズ\n",
    "\n",
    "\n",
    "$h$\n",
    " が高さ方向、 \n",
    "$w$\n",
    " が幅方向である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_func(N_in, P, F, S):\n",
    "    return int((N_in + 2*P - F)/S + 1)\n",
    "    \n",
    "Fh = 3\n",
    "Fw = 3\n",
    "Sh = 1\n",
    "Sw = 1\n",
    "Ph = 0\n",
    "Pw = 0\n",
    "\n",
    "Nh_in = 28\n",
    "Nw_in = 28\n",
    "\n",
    "Nh_out = output_func(Nh_in, Ph, Fh, Sh)\n",
    "Nw_out = output_func(Nw_in, Pw, Fw, Sw)\n",
    "\n",
    "Nh_out, Nw_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$\n",
    "\n",
    "$P_{i,j}$\n",
    " : $i$行$j$列への出力する場合の入力配列のインデックスの集合。 \n",
    "\n",
    "$S_h\n",
    "×\n",
    "S_w$\n",
    " の範囲内の行（$p$）と列（$q$）\n",
    "\n",
    "\n",
    "$S_h,S_w$\n",
    " : 高さ方向（$h$）と幅方向（$w$）のストライドのサイズ\n",
    "\n",
    "\n",
    "$(p,q)∈P_{i,j}: P_{i,j}$\n",
    " に含まれる行（$p$）と列（$q$）のインデックス\n",
    "\n",
    "\n",
    "$a_{i,j,m}$\n",
    " : 出力される配列の$i$行$j$列、$k$チャンネルの値\n",
    "\n",
    "\n",
    "$x_{p,q,k}$\n",
    " : 入力の配列の$p$行$q$列、$k$チャンネルの値\n",
    "\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス \n",
    "$(p,q)$\n",
    " を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力はNHWC(n_samples, height, width, n_channels)\n",
    "class MaxPool2D:\n",
    "    \n",
    "    def __init__(self, ch):\n",
    "        self.FH = 2\n",
    "        self.FW = 2\n",
    "        self.S = 1 # stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.A = np.zeros(x.shape)\n",
    "        \n",
    "        \n",
    "        sample = x.shape[0]\n",
    "        chanel = x.shape[1]\n",
    "        height = x.shape[2]\n",
    "        width = x.shape[3]\n",
    "        self.Z_index = np.zeros([sample,chanel,height,width])\n",
    "        Z = np.zeros([sample,chanel,int(height/2),int(width/2)])\n",
    "\n",
    "        for h in range(sample):#サンプルを設定\n",
    "            for k in range(chanel):#チャネルを設定\n",
    "                for i in range(self.FH):#高さを設定 フィルタ＝ストライド＝２\n",
    "                    for j in range(self.FW):#幅を設定\n",
    "                            Z0 = x[h,k,i*2:i*2 + 2,j*2:j*2 + 2]\n",
    "                            Z[h,k,i,j] = np.nanmax(Z0)\n",
    "                            a = Z0/np.nanmax(Z0)#最大値が１の行列\n",
    "                            self.Z_index[h,k,i*2:i*2 + 2,j*2:j*2 + 2] += (np.where(a == 1 ,1,0)) # z idx keep\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return self.A\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.flatten = x.shape\n",
    "        self.FLT_shape = x.reshape(x.shape[0], -1)[1]\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return x.reshape(self.flatten)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Xavierの初期値\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = 1/np.sqrt(n_nodes1) # self.batch_sizeかも\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class He:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Xavierの初期値\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = np.sqrt(2/n_nodes1) \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)       \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # layerの引数は\"class FCのインスタンス自体\"を取得している。\n",
    "\n",
    "        layer.B -= self.lr*layer.B_dash # B更新\n",
    "        layer.W -= self.lr*layer.W_dash # W更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes_out, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "#         self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes_out = n_nodes_out\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "    def forward(self, x, n_nodes_in):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_nodes_in = n_nodes_in\n",
    "        self.W = self.initializer.W(self.n_nodes_in, self.n_nodes_out)\n",
    "        self.B = self.initializer.B(self.n_nodes_out)\n",
    "        self.Z = x # 前のレイヤーのZを取得\n",
    "        A = x@self.W + self.B\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "#         pdb.set_trace()\n",
    "        dZ = dA@self.W.T # このレイヤーのW\n",
    "\n",
    "        # 更新\n",
    "        self.B_dash = np.mean(dA, axis=0)\n",
    "        self.W_dash = self.Z.T@dA # 前のレイヤーのZ\n",
    "        self = self.optimizer.update(self) # この引数のselfはこの\"class FCのインスタンス自体\"を取得している。\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.F = np.where(x > 0, 1, 0) # backwardの引数のためのAを保管\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x):\n",
    "#         pdb.set_trace()\n",
    "        return x*self.F\n",
    "    \n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def cross_entropy_func(self, x, y):\n",
    "        li = np.empty(len(x))\n",
    "        for i in range(len(x)):\n",
    "            lj = np.empty(10) # 10 = self.n_output, 最終的に変数にする\n",
    "            for j in range(10): # 10 = self.n_output, 最終的に変数にする\n",
    "                lj[j] = y[i,j]*np.log(x[i,j])\n",
    "            li[i] = sum(lj)\n",
    "        self.l = sum(li)/-10 # 10 = self.n_output, 最終的に変数にする\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_max = np.max(x, axis=1)\n",
    "        exp_x = np.exp(x - x_max.reshape(-1, 1))\n",
    "        sum_exp_x = np.sum(exp_x, axis=1).reshape(-1, 1)  \n",
    "        return exp_x/sum_exp_x\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        self.cross_entropy_func(x, y)\n",
    "        return x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=42):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_im2col:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, FN, FC, FH, FW, lr, stride=1, padding=0, sigma=0.01):\n",
    "        self.FN = FN\n",
    "        self.FC = FC\n",
    "        self.FH = FH\n",
    "        self.FW = FW\n",
    "        self.lr = lr\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        self.w = sigma * np.random.randn(self.FN, self.FC, self.FH, self.FW)\n",
    "        self.b = np.zeros(self.FN)\n",
    "        \n",
    "    # im２col利用したforward関数\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        ２次元畳み込みフォアードの関数\n",
    "        ---------------\n",
    "        parameter\n",
    "        ---------------\n",
    "        X: 入力データ\n",
    "        N: 入力データの個数\n",
    "        H: 入力データの高さ\n",
    "        W:入力データの幅\n",
    "        i :　配列行方向インデックス\n",
    "        j:　配列列方向のインデックス\n",
    "        m:　出力チャネルのインデックス\n",
    "        C:　入力チャネル数\n",
    "        FN: フィルタのこすう\n",
    "        FH:　フィルタの高さ\n",
    "        FW:　フィルタの幅\n",
    "        FC:　フィルタのチャネル\n",
    "        W: フィルタ\n",
    "        b：　バイアス\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        \n",
    "        #各データのチャネル、高さ、幅の取り出し\n",
    "        self.N ,self.C, self.H, self.W = X.shape\n",
    "        self.FN, self.FC, self.FH, self.FW = self.w.shape\n",
    "        OH, OW = self.N_out_size(X, self.w,  self.pad, self.stride)\n",
    "\n",
    "        # im2col(x, FH, FW, stride, padding)\n",
    "        self.col = self.im2col(self.X, self.FH, self.FW, self.stride, self.pad)\n",
    "        self.col_w = self.w.reshape(self.FN, -1).T\n",
    "        out = np.dot(self.col, self.col_w) + self.b\n",
    "        out = out.reshape(self.N, OH, OW, -1).transpose(0, 3, 1, 2)\n",
    "        return out\n",
    "        \n",
    "    # backward関数\n",
    "    def backward(self, n_out):\n",
    "        \"\"\"\n",
    "        ２次元畳み込みバックプロバゲーションの関数\n",
    "        ---------------\n",
    "        parameter\n",
    "        ---------------\n",
    "        n_out:　入力データ\n",
    "\n",
    "        ---------------\n",
    "        return\n",
    "        ---------------\n",
    "        dx: デルタX値\n",
    "        \"\"\"\n",
    "        n_out = n_out.transpose(0, 2, 3, 1).reshape(-1, self.FN)\n",
    "\n",
    "        self.db = np.sum(n_out, axis=0)\n",
    "        self.dw = np.dot(self.col.T, n_out)\n",
    "        self.dw = self.dw.transpose(1, 0).reshape(self.FN, self.FC, self.FH, self.FW)\n",
    "\n",
    "        dcol = np.dot(n_out, self.col_w.T)\n",
    "        self.dx = self.col2im(dcol, self.X.shape, self.FH, self.FW, self.stride, self.pad)\n",
    "        self.w, self.b = self._update()\n",
    "        return self.dx\n",
    "    \n",
    "    # update関数\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        フィルタとバイアス更新式\n",
    "        ----------------\n",
    "        parameter\n",
    "        ----------------\n",
    "        lr: 学習率\n",
    "        w：フィルタ\n",
    "        b：バイアス\n",
    "        ---------------\n",
    "        return\n",
    "        ---------------\n",
    "        w ： 更新後のW \n",
    "        b ：　更新後のb\n",
    "        \"\"\"\n",
    "        self.w = self.w  - self.lr * self.dw\n",
    "        self.b = self.b -self. lr * self.db\n",
    "\n",
    "        return  self.w, self.b \n",
    "    \n",
    "    def N_out_size(self,N, F, P, S):\n",
    "        \"\"\"\n",
    "        出力データのサイズを導く関数\n",
    "        -------------------\n",
    "        parameter\n",
    "        -------------------\n",
    "        N:　入力データ\n",
    "        Nh：入力データの高さ\n",
    "        Nw：　入力データの幅\n",
    "        F：　filter\n",
    "        Fh：フィルタの高さ\n",
    "        Fw：　フィルタの幅\n",
    "        P：　パディング\n",
    "        S: ストライド\n",
    "        Sh：ストライドの高さ方向\n",
    "        Sw：ストライドの幅方向\n",
    "        ------------------\n",
    "        return\n",
    "        ------------------\n",
    "        Nh_out：出力データの高さ\n",
    "        Nw_out：　出力データの幅\n",
    "        \"\"\"\n",
    "        _, _, NH, NW = N.shape\n",
    "        _, _, FH , FW = F.shape\n",
    "\n",
    "        NH_out = (NH + 2*P - FH) // S +1\n",
    "        NW_out = (NW + 2*P - FW) // S +1 \n",
    "        return NH_out,  NW_out\n",
    "        \n",
    "    # im2col関数\n",
    "    def im2col(self, input_data, FH, FW, stride, pad):\n",
    "        N, C, H, W = input_data.shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "\n",
    "        # paddingの追加\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        # データの空箱作成\n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        return col      \n",
    "\n",
    "    #col2im\n",
    "    def col2im(self,col, input_shape, FH, FW, stride=1, pad=0):\n",
    "        N, C, H, W = input_shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride -1, W+ 2*pad+stride-1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScratchCNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScratchCNNClassifier:\n",
    "\n",
    "#     def __init__(self, verbose=None, batch_size=20, a=0.01, n_epoch=1):\n",
    "#         self.verbose = verbose\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lr = a\n",
    "#         self.n_epoch = n_epoch\n",
    "\n",
    "\n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "# # 初期値\n",
    "#         NODE_TO_SOFTMAX = 10 # softmaxに送るノード数（固定）\n",
    "#         self.sigma = 0.01 # ガウス分布の標準偏差\n",
    "#         self.x = X\n",
    "        \n",
    "# # X設定　（60000, 1, 28, 28）の場合想定        \n",
    "# #         self.n_samples, self.input_ch, self.height, self.width = X.shape        \n",
    "#         self.n_samples = X.shape[0]        \n",
    "#         self.input_ch = X.shape[1]        \n",
    "#         self.height = X.shape[2]        \n",
    "#         self.width = X.shape[3]        \n",
    "        \n",
    "# # Bias\n",
    "#         self.B_1 = X.shape[1] # Bのシェイプ（output_ch）\n",
    "        \n",
    "# # fitメソッド内        \n",
    "#         self.Conv2d1 = Conv2d_im2col(FN=32, FC=1, FH=3, FW=3, lr = self.lr)\n",
    "#         self.activation1 = ReLU()\n",
    "#         self.MaxPool2D_1 = MaxPool2D(20)\n",
    "#         self.Flatten = Flatten()\n",
    "#         optimizer = SGD(self.lr)\n",
    "#         self.FC1 = FC(NODE_TO_SOFTMAX, SimpleInitializer(self.sigma), optimizer)\n",
    "#         self.SoftMax = Softmax()\n",
    "        \n",
    "#         get_mini_batch = GetMiniBatch(X, y, batch_size = self.batch_size)\n",
    "#         self.l_list = np.empty(self.n_epoch)\n",
    "#         self.l_val_list = np.empty(self.n_epoch)\n",
    "#         self.cnt = 0\n",
    "#         self.cnt_list = []\n",
    "#         CNT = 0\n",
    "#         for i in range(self.n_epoch):\n",
    "#             for X, y in get_mini_batch:        \n",
    "        \n",
    "        \n",
    "# # イテレーションごとのフォワード  \n",
    "#                 # Input Image (20, 1, 28, 28)\n",
    "#                 # print(X.shape)\n",
    "#                 FM1 = self.Conv2d1.forward(X) # (20, 32, 26, 26)\n",
    "#                 # print(FM1.shape)\n",
    "#                 Z1 = self.activation1.forward(FM1) # (20, 32, 26, 26)\n",
    "#                 # print(Z1.shape)\n",
    "#                 FM2 = self.MaxPool2D_1.forward(Z1) # (20, 32, 13, 13)\n",
    "#                 # print(FM2.shape)\n",
    "#                 FLT = self.Flatten.forward(FM2) # (20, 5408)\n",
    "#                 # print(FLT.shape)\n",
    "#                 n_node_in = FLT.shape[1]\n",
    "#                 A1 = self.FC1.forward(FLT, n_node_in) # (20, 10) \n",
    "#                 # print(A1.shape)\n",
    "#                 Z2 = self.SoftMax.forward(A1) # (20, 10) \n",
    "#                 # print(Z2.shape)\n",
    "#                 # pdb.set_trace()\n",
    "# # イテレーションごとのバックワード\n",
    "#                 dA1 = self.SoftMax.backward(Z2, y) # (20, 10) \n",
    "#                 # print(dA1.shape)\n",
    "#                 # pdb.set_trace()\n",
    "#                 dFLT = self.FC1.backward(dA1) # (20, 5408)\n",
    "#                 # print(dFLT.shape)\n",
    "#                 dFM2 = self.Flatten.backward(dFLT) # (20, 32, 13, 13)\n",
    "#                 # print(dFM2.shape)\n",
    "#                 dZ1 = self.MaxPool2D_1.backward(dFM2) # (20, 32, 26, 26)\n",
    "#                 # print(dZ1.shape)\n",
    "#                 dFM1 = self.activation1.backward(dZ1) # (20, 32, 26, 26)\n",
    "#                 # print(dFM1.shape)\n",
    "#                 dZ0 = self.Conv2d1.backward(dFM1) # (20, 1, 28, 28) ＊dZ0は使用しない        \n",
    "                \n",
    "# #                 print(CNT)\n",
    "# #                 CNT+=1\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#             サンプル\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#             次の形のndarray, shape (n_samples, 1)\n",
    "#             推定結果\n",
    "#         \"\"\"\n",
    "#         A1 = self.Conv2d1.forward(X) # (26, 26, 32)\n",
    "#         A2 = self.activation1.forward(A1) # (13, 13, 32)\n",
    "#         A3 = self.MaxPool2D_1.forward(A2) # (11, 11, 64)\n",
    "#         A4 = self.Flatten.forward(A3) # (5, 5, 64)\n",
    "#         n_node_in = A4.shape[1]\n",
    "#         A5 = self.FC1.forward(A4, n_node_in) # (3, 3, 128)\n",
    "#         A6 = self.SoftMax.forward(A5) # (1, 1, 128)\n",
    "#         result = np.argmax(A6, axis=1)\n",
    "#         print(A6[0:10])\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクラッチのCNN\n",
    "class ScratchCNNClassifier2:\n",
    "    \"\"\"\n",
    "    n_iter :学習回数\n",
    "    lr: 学習率\n",
    "    sigma:初期値\n",
    "    batch:バッチサイズ\n",
    "    loss:lossの保管\n",
    "    val_loss:バリデーションのloss保管\n",
    "    filter_output:アウトプットの値\n",
    "    filter_channel:フィルタのチャネル数\n",
    "    filter_size:フィルタのサイズ\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01,sigma=0.1, n_iter=3, batch=20, filter_output=30, filter_channel=1, filter_size=3, vervose=False):\n",
    "        # 　学習回数\n",
    "        self.iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.batch = batch\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.filter_output = filter_output\n",
    "        self.filter_channel = filter_channel\n",
    "        self.filter_size = filter_size\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y,  X_val=None, y_val=None):\n",
    "        self.n_features = X.shape[0]\n",
    "        self.n_output = 10\n",
    "        # 入力用のデータ\n",
    "        input_size = 28\n",
    "        padding = 0\n",
    "        stride = 1\n",
    "        filter_num = 30\n",
    "        conv_output_size = (input_size - self.filter_size + 2 * padding) / stride + 1\n",
    "        pool_output_size = int(self.filter_output * (conv_output_size / 2) * (conv_output_size / 2))\n",
    "\n",
    "\n",
    "        #更新方法の定義\n",
    "        self.optimizer = AdaGrad(self.lr, self.batch)\n",
    "        #　畳み込み層の定義\n",
    "        self.Conv2d = Conv2d_im2col(FN=30, FC=1, FH=3, FW=3, lr = self.lr)\n",
    "        # １層目出力データの活性化の定義\n",
    "        self.Conv_activation = Relu()\n",
    "        # pooling処理の定義\n",
    "        self.Pool = Maxpool2D_im2col()\n",
    "        #　平滑化処理の定義\n",
    "        self.flatten = Flatten()\n",
    "        # 全結合層の定義\n",
    "        self.FC1 = FC(18750, self.n_output, HeInitializer(self.filter_output), self.optimizer)\n",
    "\n",
    "        # 最終層の活性化の定義\n",
    "        self.activation1 = Softmax()\n",
    "\n",
    "        # 学習開始\n",
    "        for i in range(self.iter):\n",
    "            #  バッチサイズのカウント\n",
    "            count = 0\n",
    "            history = np.zeros(X.shape[0] // self.batch)\n",
    "            if X_val:\n",
    "                val_history = np.zeros(X_val.shape[0] // self.batch)\n",
    "            #　ミニバッチの取り出しコード\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # forward\n",
    "                A0 = self.Conv2d.forward(mini_X_train)#畳み込み層\n",
    "                Z0 = self.Conv_activation.forward(A0)#RELUで活性化\n",
    "                P0 = self.Pool.forward(Z0)#pooling \n",
    "                F0 = self.flatten.forward(P0)#flattenして平滑化\n",
    "                A1 = self.FC1.forward(F0)#全結合層\n",
    "                Z1 = self.activation1.forward(A1)#Softmaxで活性化\n",
    "\n",
    "                # backward\n",
    "                dA1, history[count] = self.activation1.backward(Z1, mini_y_train)#Softmaxのバック\n",
    "                #flatten元の形に戻す\n",
    "                dZ1 = self.FC1.backward(dA1)#全結合層のバック\n",
    "                #flatten元の形に戻す\n",
    "                dF0 = self.flatten.backward(dZ1)\n",
    "                #pooling層のバック\n",
    "                dP0 = self.Pool.backward(dF0)\n",
    "                dA0 = self.Conv_activation.backward(dP0)#RELUのバック\n",
    "                dZ0 = self.Conv2d.backward(dA0)#畳み込み層のバック\n",
    "                count += 1\n",
    "            \n",
    "            #loss保管のため学習したパラメータでforward\n",
    "            A0 = self.Conv2d.forward(X)#畳み込み層\n",
    "            Z0 = self.Conv_activation.forward(A0)#RELUで活性化\n",
    "            P0 = self.Pool.forward(Z0)#pooling \n",
    "            F0 = self.flatten.forward(P0)#flattenして平滑化\n",
    "            A1 = self.FC1.forward(F0)#全結合層\n",
    "            Z1 = self.activation1.forward(A1)#Softmaxで活性化\n",
    "            \n",
    "            _, history= self.activation1.backward(Z1, y)\n",
    "            self.loss[i] = history\n",
    "            \n",
    "            # X_valあった場合の処理\n",
    "            if np.any(X_val): \n",
    "                A0 = self.Conv2d.forward(X_val)\n",
    "                Z0 = self.Conv_activation.forward(A0)\n",
    "                P0 = self.Pool.forward(Z0)#pooling \n",
    "                F0 = self.flatten.forward(P0)#flattenして平滑化\n",
    "                A1 = self.FC1.forward(F0)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                # 学習記録\n",
    "                _, val_history = self.activation1.backward(Z1, y_val)\n",
    "                self.val_loss[i] = val_history\n",
    "            \n",
    "            \n",
    "    # 推定\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        print(X.shape)\n",
    "        A0 = self.Conv2d.forward(X)#畳み込み層\n",
    "        Z0 = self.Conv_activation.forward(A0)#RELUで活性化\n",
    "        P0 = self.Pool.forward(Z0)#pooling \n",
    "        F0 = self.flatten.forward(P0)#flattenして平滑化\n",
    "        A1 = self.FC1.forward(F0)#全結合層\n",
    "        Z1 = self.activation1.forward(A1)#Softmaxで活性化\n",
    "        return np.argmax(Z1, axis= 1)\n",
    "    \n",
    "    \n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.w = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.b =  initializer.B(self.n_nodes2)\n",
    "        # adagrad用\n",
    "        #if Adagrad = True: \n",
    "        self.h_w = np.zeros((n_nodes1, n_nodes2))\n",
    "        self.h_b = np.zeros(n_nodes2)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"     \n",
    "        self.Z = X\n",
    "        \n",
    "        self.A =  (X @ self.w) + self.b\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dw = self.Z.T @ dA \n",
    "        dZ = dA @ self.w.T\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ    \n",
    "\n",
    "    \n",
    "#初期化\n",
    "class HeInitializer():\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = np.sqrt(2 / sigma)\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        self.w = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return self.w\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        self.b = self.sigma * np.random.randn(n_nodes2)\n",
    "        return self.b\n",
    "\n",
    "    \n",
    "# 活性化関数\n",
    "class Softmax():\n",
    "    def forward(self, A):\n",
    "        # overflow対策\n",
    "        A_max = np.max(A, axis=1)\n",
    "        exp_A = np.exp(A - A_max.reshape(-1, 1))\n",
    "        sum_exp_A = np.sum(exp_A, axis=1).reshape(-1, 1)\n",
    "        return exp_A / sum_exp_A\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        loss = self._cross_entropy(Z, Y)\n",
    "        D  = Z - Y\n",
    "        return D, loss \n",
    "    \n",
    "    def _cross_entropy(self,Z, y):\n",
    "        \"\"\"\n",
    "        パラメータ\n",
    "        ーーーーーーーーーーーーー\n",
    "        y : 正解ラベル（one-hot表現）\n",
    "        z３　：　クラスの確率\n",
    "        n : バッチサイズ\n",
    "\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:    # 次元が 1 の場合\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -1* np.sum(y * np.log(Z)) / batch_size\n",
    "\n",
    "\n",
    "# 活性化関数\n",
    "class Relu():\n",
    "    def forward(self, A):\n",
    "        self.mask = (A <= 0)\n",
    "        a = A.copy()\n",
    "        self.A = A\n",
    "        a[self.mask] = 0\n",
    "        return np.maximum(0, A)\n",
    "      \n",
    "    def backward(self, dA):\n",
    "        dA[self.mask] = 0\n",
    "        out = dA\n",
    "        return out\n",
    "\n",
    "    \n",
    "# AdaGrad\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, batch):\n",
    "        self.lr = lr\n",
    "        self.batch = batch\n",
    "        # 過去の勾配の２乗和の保管用\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.h_w += (layer.dw/ self.batch) * (layer.dw/ self.batch)\n",
    "        layer.h_b += (layer.db/self.batch) * (layer.db / self.batch)\n",
    "        layer.w -= self.lr * (1 / (np.sqrt(layer.h_w) + 1e-07)) * (layer.dw/ self.batch)\n",
    "        layer.b -= self.lr * (1 / (np.sqrt(layer.h_b) + 1e-07)) * (layer.db/ self.batch)\n",
    "        \n",
    "#         layer.h_W += layer.dW * layer.dW\n",
    "#         layer.h_B += layer.dB * layer.dB\n",
    "#         layer.W -= self.lr * (1 / (np.sqrt(layer.h_W) + 1e-07)) * layer.dW\n",
    "#         layer.B -= self.lr * (1 / (np.sqrt(layer.h_B) + 1e-07)) * layer.dB\n",
    "        return layer\n",
    "                                 \n",
    "    \n",
    "#　畳み込み層  \n",
    "class Conv2d_im2col:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, FN, FC, FH, FW, lr, stride=1, padding=0, sigma=0.01):\n",
    "        self.FN = FN\n",
    "        self.FC = FC\n",
    "        self.FH = FH\n",
    "        self.FW = FW\n",
    "        self.lr = lr\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        self.w = sigma * np.random.randn(self.FN, self.FC, self.FH, self.FW)\n",
    "        self.b = np.zeros(self.FN)\n",
    "        \n",
    "    # im２col利用したforward関数\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        ２次元畳み込みフォアードの関数\n",
    "        ---------------\n",
    "        parameter\n",
    "        ---------------\n",
    "        X: 入力データ\n",
    "        N: 入力データの個数\n",
    "        H: 入力データの高さ\n",
    "        W:入力データの幅\n",
    "        i :　配列行方向インデックス\n",
    "        j:　配列列方向のインデックス\n",
    "        m:　出力チャネルのインデックス\n",
    "        C:　入力チャネル数\n",
    "        FN: フィルタのこすう\n",
    "        FH:　フィルタの高さ\n",
    "        FW:　フィルタの幅\n",
    "        FC:　フィルタのチャネル\n",
    "        W: フィルタ\n",
    "        b：　バイアス\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        \n",
    "        #各データのチャネル、高さ、幅の取り出し\n",
    "        self.N ,self.C, self.H, self.W = X.shape\n",
    "        self.FN, self.FC, self.FH, self.FW = self.w.shape\n",
    "        OH, OW = self.N_out_size(X, self.w,  self.pad, self.stride)\n",
    "\n",
    "        # im2col(x, FH, FW, stride, padding)\n",
    "        self.col = self.im2col(self.X, self.FH, self.FW, self.stride, self.pad)\n",
    "        self.col_w = self.w.reshape(self.FN, -1).T\n",
    "        out = np.dot(self.col, self.col_w) + self.b\n",
    "        out = out.reshape(self.N, OH, OW, -1).transpose(0, 3, 1, 2)\n",
    "        return out\n",
    "        \n",
    "    # backward関数\n",
    "    def backward(self, n_out):\n",
    "        \"\"\"\n",
    "        ２次元畳み込みバックプロバゲーションの関数\n",
    "        ---------------\n",
    "        parameter\n",
    "        ---------------\n",
    "        n_out:　入力データ\n",
    "\n",
    "        ---------------\n",
    "        return\n",
    "        ---------------\n",
    "        dx: デルタX値\n",
    "        \"\"\"\n",
    "        n_out = n_out.transpose(0, 2, 3, 1).reshape(-1, self.FN)\n",
    "\n",
    "        self.db = np.sum(n_out, axis=0)\n",
    "        self.dw = np.dot(self.col.T, n_out)\n",
    "        self.dw = self.dw.transpose(1, 0).reshape(self.FN, self.FC, self.FH, self.FW)\n",
    "\n",
    "        dcol = np.dot(n_out, self.col_w.T)\n",
    "        self.dx = self.col2im(dcol, self.X.shape, self.FH, self.FW, self.stride, self.pad)\n",
    "        self.w, self.b = self._update()\n",
    "        return self.dx\n",
    "    \n",
    "    # update関数\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        フィルタとバイアス更新式\n",
    "        ----------------\n",
    "        parameter\n",
    "        ----------------\n",
    "        lr: 学習率\n",
    "        w：フィルタ\n",
    "        b：バイアス\n",
    "        ---------------\n",
    "        return\n",
    "        ---------------\n",
    "        w ： 更新後のW \n",
    "        b ：　更新後のb\n",
    "        \"\"\"\n",
    "        self.w = self.w  - self.lr * self.dw\n",
    "        self.b = self.b -self. lr * self.db\n",
    "\n",
    "        return  self.w, self.b \n",
    "    \n",
    "    def N_out_size(self,N, F, P, S):\n",
    "        \"\"\"\n",
    "        出力データのサイズを導く関数\n",
    "        -------------------\n",
    "        parameter\n",
    "        -------------------\n",
    "        N:　入力データ\n",
    "        Nh：入力データの高さ\n",
    "        Nw：　入力データの幅\n",
    "        F：　filter\n",
    "        Fh：フィルタの高さ\n",
    "        Fw：　フィルタの幅\n",
    "        P：　パディング\n",
    "        S: ストライド\n",
    "        Sh：ストライドの高さ方向\n",
    "        Sw：ストライドの幅方向\n",
    "        ------------------\n",
    "        return\n",
    "        ------------------\n",
    "        Nh_out：出力データの高さ\n",
    "        Nw_out：　出力データの幅\n",
    "        \"\"\"\n",
    "        _, _, NH, NW = N.shape\n",
    "        _, _, FH , FW = F.shape\n",
    "\n",
    "        NH_out = (NH + 2*P - FH) // S +1\n",
    "        NW_out = (NW + 2*P - FW) // S +1 \n",
    "        return NH_out,  NW_out\n",
    "        \n",
    "    # im2col関数\n",
    "    def im2col(self, input_data, FH, FW, stride, pad):\n",
    "        N, C, H, W = input_data.shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "\n",
    "        # paddingの追加\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        # データの空箱作成\n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        return col      \n",
    "\n",
    "    #col2im\n",
    "    def col2im(self,col, input_shape, FH, FW, stride=1, pad=0):\n",
    "        N, C, H, W = input_shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride -1, W+ 2*pad+stride-1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "    \n",
    "    \n",
    "# im2col版Maxpooling\n",
    "class Maxpool2D_im2col:\n",
    "    def __init__(self, pool_size=(2,2), stride=1, padding=0):\n",
    "        self.PH,  self.PW = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        parameter\n",
    "        --------------\n",
    "        N：入力データのこすう\n",
    "        C：入力データのチャネル数\n",
    "        H：入力データの高さ\n",
    "        W：入力データの幅\n",
    "        \n",
    "        return\n",
    "        --------------\n",
    "        out : maxpooling\n",
    "        \n",
    "        \"\"\"\n",
    "        self.X_shape = X.shape\n",
    "        self.N, self.C, self.H, self.W = self.X_shape\n",
    "        out_h = (self.H + 2*self.pad - self.PH) // self.stride + 1\n",
    "        out_w = (self.W + 2*self.pad - self.PW)// self.stride + 1\n",
    "        col = self.im2col(X, self.PH, self.PW, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.PH*self.PW)\n",
    "        \n",
    "        \n",
    "        index_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(self.N, out_h, out_w, self.C).transpose(0, 3, 1, 2)\n",
    "        self.index_max = index_max\n",
    "        return out \n",
    "    \n",
    "    def backward(self, n_out):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        n_out = n_out.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.PH * self.PW\n",
    "        out_max = np.zeros((n_out.size, pool_size))\n",
    "        out_max[np.arange(self.index_max.size), self.index_max.flatten()] = n_out.flatten()\n",
    "        out_max = out_max.reshape(n_out.shape + (pool_size,))\n",
    "        \n",
    "        dcol = out_max.reshape(out_max.shape[0] * out_max.shape[1] * out_max.shape[2], -1)\n",
    "        dx = self.col2im(dcol, self.X_shape, self.PH, self.PW, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    # im2col関数\n",
    "    def im2col(self, input_data, FH, FW, stride, pad):\n",
    "        N, C, H, W = input_data.shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "\n",
    "        # paddingの追加\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        # データの空箱作成\n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        return col  \n",
    "    \n",
    "    #col2im\n",
    "    def col2im(self, col, input_shape, FH, FW, stride=1, pad=0):\n",
    "        N, C, H, W = input_shape\n",
    "        #出力データのサイズ\n",
    "        out_h = (H + 2*pad - FH)//stride + 1\n",
    "        out_w = (W + 2*pad - FW)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride -1, W+ 2*pad+stride-1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "    \n",
    "    \n",
    "# 平滑化   \n",
    "class Flatten:\n",
    "    \"\"\"\n",
    "    平滑化クラス    \n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        a : 出力データ\n",
    "        \n",
    "        \"\"\"\n",
    "        # 出力チャンネル数と画像サイズに該当する次元を減らす\n",
    "        self.x_shape = x.shape\n",
    "        \n",
    "        x = x.reshape(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        da : 後ろから流れてきた勾配\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dz : 前に流す勾配\n",
    "        \n",
    "        \"\"\"\n",
    "        # プーリング層へ流すために、forward時の元の次元数に戻す\n",
    "        dz = da.reshape(self.x_shape)\n",
    "        \n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScratchCNNClassifier2()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(y_pred)\n",
    "print(y_true)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "* 入力サイズ : 144×144, 3チャンネル\n",
    "* フィルタサイズ : 3×3, 6チャンネル\n",
    "* ストライド : 1\n",
    "* パディング : なし\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "* 入力サイズ : 60×60, 24チャンネル\n",
    "* フィルタサイズ : 3×3, 48チャンネル\n",
    "* ストライド　: 1\n",
    "* パディング : なし\n",
    "\n",
    "3.\n",
    "\n",
    "\n",
    "* 入力サイズ : 20×20, 10チャンネル\n",
    "* フィルタサイズ: 3×3, 20チャンネル\n",
    "* ストライド : 2\n",
    "* パディング : なし\n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
