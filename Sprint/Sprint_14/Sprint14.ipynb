{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バウンディングボックス\n",
    "* クラスと物体を含む矩形のこと。\n",
    "* まずバウンディングボックスを予測する必要がある。\n",
    "\n",
    "ローカライゼーション\n",
    "* 物体の位置を特定する事\n",
    "\n",
    "物体検知output\n",
    "1. 入力画像から固定サイズのウィンドウをすべての可能な位置で取得する\n",
    "2. これらのパッチ(領域)を画像分類器に入力し、分類問題としてモデル化する。\n",
    "\n",
    "ダウンサンプリング\n",
    "1. 元の画像を複数サイズにサイズ変更した画像を用意する。\n",
    "1. これらの画像のいずれかで、選択したサイズのウィンドウ内に物体が完全に含められる。\n",
    "1. 一般的には、特定の条件(典型的には最小サイズに達する)が得られるまで、画像のダウンサンプリング(サイズが縮小)を行う。\n",
    "1. これらの画像のそれぞれについて、 固定サイズのウィンドウ検出を実行する。\n",
    "1. ピラミッド型の構造になる。\n",
    "1. このようなピラミッドには64レベルまでのレベルがあるのが一般的。\n",
    "1. これらのウィンドウは全て、関心のあるオブジェクトを検出するために 分類器に入力され、サイズと場所の問題を解決します。\n",
    "\n",
    "アスペクト比\n",
    "1. もう1つ、縦横比(アスペクト比、aspect ratio)問題がある。\n",
    "1. 座っている人が立っている人や眠っている人とは異なるアスペクト比を持つように、多く􏰃物体が さまざまな形で存在する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**領域提案**（オブジェクトが含まれている可能性のあるイメージ内の領域）を検出する。\n",
    "\n",
    "->Edge Boxes[1] などのアルゴリズムを使用して領域提案を生成する。\n",
    "\n",
    "\n",
    "領域提案から CNN 特徴量を抽出。\n",
    "\n",
    "->提案領域はイメージからトリミングされ、サイズ変更される。\n",
    "\n",
    "->その後、トリミングされてサイズ変更された領域は、CNN によって分類される。\n",
    "\n",
    "抽出した特徴量を使用してオブジェクトを分類する。\n",
    "\n",
    "->最後に、CNN 特徴量を使用して学習したサポート ベクター マシン (SVM) によって、領域提案境界ボックスが調整される。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "RPNは小さなニューラルネットワークで、特徴マップ上をn×nサイズのスライディングwindowで走査し、各々のn×nサイズの注目領域をネットワークの入力とする。そして、各スライディングwindow位置に対してk個の候補領域を推定する。\n",
    "\n",
    "k個の候補領域を推定するために、RPNは以下2つの全結合層へ分岐する。\n",
    "\n",
    "cls layer：物体かどうか(objectness)を分類する\n",
    "reg layer：Bounding Boxの回帰を行う\n",
    "\n",
    "cls layerには、k個の各候補領域がオブジェクトか、背景かの確率を推定した2k個のスコアが出力される。\n",
    "reg layerには、k個のBounding Boxの座標・サイズを表す4k個の出力がある。(x座標、y座標、幅、高さ)\n",
    "\n",
    "この論文ではn=3とし、実際にはスライディングwindowではなく3×3サイズの畳み込み層と、それに続く2つに分岐した1×1サイズの畳み込み層(Fully Convolutional Networks)として実装している。\n",
    "(これは3×3のスライディングwindowと等価)\n",
    "\n",
    "\n",
    "「特徴マップを抽出するCNNをRPNも共有している」RPN自体も畳み込みニューラルネットワークの構造を持っている\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Region Propsal Network(RPN)はCNNの特徴マップ（つまりResnet等のCNN層出力）を入力。\n",
    "\n",
    "ある領域が物体か背景か(objectness)およびアンカーの位置の補正データ(corrdinates)を出力する。\n",
    "objectnessは0-1の値で1に近いほど物体である確証が高い。\n",
    "corrdinatesはBindingBoxの四角の座標について補正する量を出力する。\n",
    "\n",
    "RPNは一箇所につきk個のBindingBoxを提案する。\n",
    "これは一つの物体について重複することが多く、最も確度の高い高いBindingBoxのみ残したい（refining)。大体k=128などが使われることがある通り、refineなしでは大量のpredicitionが生成されてしまい実用的でない。\n",
    "\n",
    "Refineに使われる概念がAnchorである。\n",
    "物体検出において物体は3x3のような正方形であることは少ない。\n",
    "\n",
    "例えば人間はだいたい縦長であったり、車は横長で有ることが多い。このような図形情報を扱うために提案された概念がAnchorである。\n",
    "座標に付き提案するBindingboxに対しAnchor情報とどれくらい異なるかを導出し、一番Anchorに近いBindingBoxをMain Predictionとして出力する。これによって物体一つにつきBindingBoxを一つに絞り込み、かつ一番高精度なものを残すことができる。\n",
    "\n",
    "\n",
    "Fast R-CNN に Region Proposal Network を導入して、更に高速化したアルゴリズムCNN の畳み込み層を使ってFeature Map を生成する\n",
    "Sliding Window で切り出したFeature Map から領域候補を生成\n",
    "論文では Feature Map の各点に対して、９種類のサイズの Anchor Box を定義している\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN \n",
    "物体検出􏰃タスクに対しても CNNのアルゴリズムを上手く応用できないか?という課題を解く先駆けとなった論文\n",
    "\n",
    "物体検出はより正確な畳み込みニューラルネットワークに基づく分類器に置き換えられまたが、 CNNは非常に遅く、 計算上コストが高く、スライディングウィンドウ検出器によって生成された非常に多くのパッチで CNNを実行することは不可能だった問題を**Selective Search**と呼ばれる物体候補 (object proposal)アルゴリズムを使用することで解決した。\n",
    "\n",
    "画像を取り込み、画像内の主要な物体を**bounding box**を介して、正確に特定する。\n",
    "\n",
    "\n",
    "### アルゴリズム\n",
    "1. 画像を入力\n",
    "1. **Selective Search**を用いて**領域候補 (Region Proposals)**を2000個程度切り出す。\n",
    "   * Selective Searchでは複数のスケールのウィンドウを調べ、テクスチャ、色、または強度を共有する隣接ピクセルを探し物体を識別します。 \n",
    "   * 画像の中からたくさんのボックスの候補をリストアップし (**region proposal**)、 それらをどれかが実際に物体に対応しているかどうかを調べる。\n",
    "1. 領域候補の領域画像を 全て一定の大きさの正方形にリサイズして CNNにかけて特徴量を取り出す\n",
    "1. 1. 取り出した特徴量を使って複数のSVMによって学習し分類を推定する。\n",
    "   1. 取り出した特徴量を使って回帰によりバウンディングボックスの正確な位置を推定する。\n",
    "1. 出力:画像内の各物体の境界ボックス +ラベル\n",
    "\n",
    "\n",
    "### 欠点\n",
    "\n",
    "* 学習を各目的ごとに別々に学習する必要がある\n",
    "  * CNNのFine-tune\n",
    "  * 複数のSVMによるクラス分類 (Classification)\n",
    "  * 物体􏰃詳細位置推定 (Bounding Box Regression)\n",
    "* 実行時間がすごく遅い :GPUを使って10-45 s/image\n",
    "\n",
    "![](https://jp.mathworks.com/help/vision/ug/rcnn_ja_JP.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPP-net(spatial pyramid pooling)\n",
    "\n",
    "### 従来手法の問題\n",
    "Selective Search によって生成された2000の領域候補(region proposal)上でCNNを実行すると、 多くの時間がかかる。\n",
    "\n",
    "\n",
    "### 問題の改善\n",
    "その領域に対応する**最後の畳み込み層の特徴マップのその部分だけにプーリング操作を実行する**ことで実現。\n",
    "\n",
    "画像1枚から一回のCNNで大きな特徴マップを作成した後、 領域候補の特徴を SPPによってベクトル化し、スピードは GPU上にて24-102倍に高速化を実現。 (2000個の領域候補はかなり領域の重複が多いため、重複する画像領域を CNNで特徴抽出するのはかなり無駄)\n",
    "\n",
    "中間層で起こるダウンサンプリング( VGGの場合の座標を単に 16で割る)を考慮に入れて、 畳み込み層上の領域を投影することによって、領域に対応する畳み込み層の矩形部分の計算可能。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN \n",
    "* 領域提案をイメージ全体で処理します。\n",
    "* 各領域提案に対応する CNN 特徴量をプーリングし分類する。\n",
    "* オーバーラップする領域の計算を共有するので、R-CNN よりも効率的。\n",
    "* 関数 trainFastRCNNObjectDetector を使用して Fast R-CNN オブジェクト検出器を学習させます。\n",
    "* この関数は、イメージからオブジェクトを検出する fastRCNNObjectDetector を返します。\n",
    "* 実はRegion Proposal の生成に時間がかかる\n",
    "![](https://jp.mathworks.com/help/vision/ug/fast_ja_JP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n",
    "* Edge Boxes などの外部アルゴリズムを使用する代わりに、領域提案ネットワーク (RPN) を追加してネットワーク内で直接領域提案を生成する。\n",
    "* 途中までの演算を共有化(Sliding Window で切り出したFeature Map から領域候補を生成)することで追加の演算コストを最小化する\n",
    "* アンカーボックスによるオブジェクトの検出を使用する。\n",
    "* ネットワーク内で領域提案を生成する方が高速であり、データに合わせてより適切に調整されます。\n",
    "* 関数 trainFasterRCNNObjectDetector を使用して Faster R-CNN オブジェクト検出器を学習させます。\n",
    "* この関数は、イメージからオブジェクトを検出する fasterRCNNObjectDetector を返します。\n",
    "![](https://jp.mathworks.com/help/vision/ug/faster_ja_JP.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "画像を重なり合うタイルに分割する\n",
    "上記のスライディングウィンドウと同様に、イメージをより小さく、\n",
    "隣接するタイル同士が少しずつ重なりあうように分割(オーバーラップタイル)します。ここでは77個の小さな画像にしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "\n",
    "Faster R-CNNはRegionProposalもCNN化することで物体検出モデルを全てDNN化し、高速化するのがモチベーションとなっている。\n",
    "\n",
    "途中までの演算を共有化(Sliding Window で切り出したFeature Map から領域候補を生成)することで追加の演算コストを最小化する\n",
    "\n",
    "またFaster-RCNNはMulti-task lossという学習技術を使っており、RegionProposalモデルも込でモデル全体をend-to-endで学習させることに成功している。\n",
    "\n",
    "Faster-RCNNはCNN出力（特徴マップ）を元にregion proposal(物体があるっぽい領域を抽出）するモデルを構築している。\n",
    "\n",
    "実際のregion proposalは3~4層ほどのCNNで構成可能で小さい。\n",
    "\n",
    "Faster R-CNNでは、従来Selective Searchで行っていた候補領域(region proposals)の検出処理をRPN (Region Proposal Network)というニューラルネットワークに置き換えてさらなる効率化を実現した。\n",
    "\n",
    "convolution layer = detect 詳細な形状の出力\n",
    "pooling layer = detect 大雑把な形を出力\n",
    "\n",
    "### attention\n",
    "* Attentionは文章のような前後の並びが重要なデータを上手く扱う事ができる\n",
    "* 並列処理の実現や遠く離れた単語間の関連性を捉える事も可能\n",
    "* Transformerは画像、オーディオ、ビデオなどにも応用していく予定\n",
    "\n",
    "RNN(Recurrent Neural Networks)やLSTM(Long Short-Term Memory networks)やgated RNNsは機械翻訳や自然言語のモデリングなど、シーケンスな処理、つまり前後の繋がりに意味があるため順番で連続して処理する必要がある処理に良く使われます。\n",
    "\n",
    "RNNやCNNは、単語単位で文頭から順番に処理をしていきます。この順番に処理をする事が原因で、処理を並列化する事が難しくなります。さらに、文章が非常に長い場合、ニューラルネットワークは、以前の内容を忘れてしまったり、または他の位置の単語の内容と混合してしまう傾向があります。\n",
    "\n",
    "最近の研究では、因数分解や条件演算を使って計算効率を向上し、モデル性能の大幅な改善も達成されています。しかし、これらはシーケンスな処理の基本的な制約、つまり平行処理が難しい事を解決するには十分ではありません。\n",
    "\n",
    "Attentionメカニズムは、ニューラルネットワークが忘却をしてしまう問題を克服する解決策の1つです。Attentionメカニズムは入力シーケンスや出力シーケンス内の距離に関わらず文脈の依存関係をモデリング可能なのです。この特徴により、Attentionはシーケンシャル処理や伝達モデルに不可欠になっています。しかし、ほとんどの場合、RNNと連携してAttentionメカニズムは使用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文読解\n",
    "\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "### 条件\n",
    "* 答える際は論文のどの部分からそれが分かるかを書く。\n",
    "* 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "* 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 物体検出の分野にはどういった手法が存在したか。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SPPnet\n",
    "* Fast R-CNN\n",
    "\n",
    "*Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, - Abstract*\n",
    "\n",
    "[1] *K. He, X. Zhang, S. Ren, and J. Sun,“Spatial pyramid pooling in deep convolutional networks for visual recognition,” in European Conference on Computer Vision (ECCV), 2014.*\n",
    "\n",
    "[2] *R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fasterとあるが、どういった仕組みで高速化したのか。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体領域提案ネットワーク（RPN）の導入\n",
    "* 検出ネットワークと全画像の畳み込み特徴量を共有しボトルネックであった領域計算のコストをほぼゼロにした。\n",
    "\n",
    "*In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. - Abstract*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Stage\n",
    "* 物体領域検出とクラス分類を同時に行う。\n",
    "\n",
    "Two-Stage\n",
    "* 第1段階で物体領域検出を行う。\n",
    "* 第2段階でクラス分類を行う。\n",
    "\n",
    "*In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3×3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN—the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. - One-Stage Detection vs. Two-Stage Proposal + Detection.*\n",
    "\n",
    "*P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, “Overfeat: Integrated recognition, localization and detection using convolutional networks,” in International Conference on Learning Representations (ICLR), 2014.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN (Region Proposal Network)は、物体領域候補の検出を行う。画像全体からCNNで抽出した特徴マップ（full-image convolutional features）に対して、候補領域のBounding Boxと、その領域の物体らしさ(Objectness)を表すスコアを出力する。\n",
    "* Fast R-CNN に Region Proposal Network を導入して、更に高速化したアルゴリズム\n",
    "* CNN の畳み込み層で生成されたFeature Mapを転用し、Sliding Window で畳み込みを行ったFeature Mapから領域提案を生成する。\n",
    "\n",
    "*An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. - Abstract*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) RoIプーリングとは何か。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込み処理を行ったfeature mapから、region proposalにあたる部分領域をうまく「固定サイズのfeature map」として抽出する。\n",
    "\n",
    "*The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates.*\n",
    "*- 3.2 Sharing Features for RPN and Fast R-CNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Anchorのサイズはどうするのが適切か。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchorは特徴マップ上の各点となる。\n",
    "\n",
    "*An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left).  - Multi-Scale Anchors as Regression References. - 3.1.1 Anchors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PASCAL VOC\n",
    "* mAP\n",
    "\n",
    "*We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics). -  4.1 Experiments on PASCAL VOC*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
